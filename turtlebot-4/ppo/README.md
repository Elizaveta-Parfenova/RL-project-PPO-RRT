# Опсиание проекта

Проект представляет собой алгоритм обучения с подкреплением PPO с интеграцией в него алгоритма RRT* для планирования оптимального пути и внедрением потенциальных полей для расчета наград и штрафов. 

### Actor-critic

Алгоритм PPO основан на архитектуре actor-critic, то есть содержит в себе две нейронные сети. 

1. Актор (actor):
  - На вход подается состояние состояние агента
  - Имеет два скрытых слоя по 128 нейронов с функцией активации ReLu
  - На выход дает вероятности выбора каждого действия (вектор)

2. Критик (critic) имеет два вида: статичный (class StaticCritic) и динамический (class Improved Critic)
   В динамичном критике *изменить описание с учетом вечерних правок в коде*: 
    - На вход подается состояние агента
    - Имеет два скрытых слоя по 128 нейронов с функцией активации ReLu
    - На выходе оценка ценности текущего состояния (скаляр)
  
   В статичном критике (class Static Critic - не является нйероннйо сеть.ю): 
      - На вход также подается определенное состояние, которое впоследствии переводится из мировых координат в пиксельные (координаты карты)
      - На выходе оценка ценности текущего состояния (скаляр), определенная по заранее созданной на основе grid_map таблице

### Алгоритм RRT*

Алгоритм RRT* предназначен для построения оптимального пути от начального положения дл цели. В данной ситуации путь строится на основе заранее известной grid_map, но в перспективе данный алгоритм позволит строить путь в неизвестных пространствах. 

### TurtleBot environment 

Представляет собой конфигурацию среды, в которой существует робот: пространство действий, состояния, способ сканирования препятсвий с помощью лидара и камеры, определение текущего местоположения, а также система наград и штрафов. 

Награды состоят из:
  - награда\штраф от потенциальных полей
  - награда\штраф из-за следования\отклонения от оптимального пути
  - штраф за столкновение с препяствием
  - штраф за преодоление макисмального количесвта шагов 
  - награда за достижение цели



 
    

