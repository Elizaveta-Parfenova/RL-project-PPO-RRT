Проект представляет собой алгоритм обучения с подкреплением PPO с интеграцией в него алгоритма RRT* для планирования оптимального пути и внедрением потенциальных полей для расчета наград и штрафов. 

Алгоритм PPO основан на архитектуре actor-critic, то есть содержит в себе две нейронные сети. 

1. Актор (actor):
  - На вход подается состояние состояние агента
  - Имеет два скрытых слоя по 128 нейронов с функцией активации ReLu
  - На выход дает вероятности выбора каждого действия (вектор)

2. Критик (critic) имеет два вида: статичный (class StaticCritic) и динамический (class Improved Critic)
   В динамичном критике *изменить описание с учетом вечерних правок в коде*: 
    - На вход подается состояние агента
    - Имеет два скрытых слоя по 128 нейронов с функцией активации ReLu
    - На выходе оценка ценности текущего состояния (скаляр)
  
   В статичном критике (class Static Critic - не является нйероннйо сеть.ю): 
      - На вход также подается определенное состояние, которое впоследствии переводится из мировых координат в пиксельные (координаты карты)
      - 
    

